{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2dryNYAN3IFX3RoJrKsP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlagur/ProjectHanium/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic\n",
        " - 기초적인 전처리, html tag 제거(크롤링한 데이터일 시)\n",
        " - 숫자/영어/특수문자 등 필요하지 않은 언어 제거\n",
        " - Lowercasting\n",
        " - \"@%*=()/+ 와 같은 punctuation(문장부호) 제거\n",
        " ### 출처: https://ebbnflow.tistory.com/246 [삶은 확률의 구름:티스토리]"
      ],
      "metadata": {
        "id": "31_U2BGVOdVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "sw74hjTCtWfH"
      },
      "outputs": [],
      "source": [
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
        "\n",
        "\n",
        "def clean_punc(text, punct, mapping):\n",
        "    for p in mapping:\n",
        "        text = text.replace(p, mapping[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean_text(texts):\n",
        "    corpus = []\n",
        "    for i in range(0, len(texts)):\n",
        "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
        "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
        "        review = review.lower() #lower case\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
        "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
        "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
        "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
        "        corpus.append(review)\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "TohrnzovOSwW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize\n",
        "- 텍스트를 토큰 단위로 나눈다\n",
        "- 띄어쓰기"
      ],
      "metadata": {
        "id": "zoFd7nEKPLW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sG0yxXv4OZUK",
        "outputId": "612ea586-b7fa-4d89-d174-5708f26c211c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-0k_i4ww9\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-0k_i4ww9\n",
            "Requirement already satisfied: tensorflow==2.7.2 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (2.7.2+zzzcolab20220516114640)\n",
            "Requirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (3.1.0)\n",
            "Collecting argparse>=1.4.0\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.5.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (3.3.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (3.17.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.46.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.37.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.26.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.2)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (4.1.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.2.0)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pykospacing import Spacing\n",
        "spacing = Spacing()\n",
        "print(spacing(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-dWZlQNR_J3",
        "outputId": "7b84d254-98f4-4c63-b52e-9469792ff8b5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김형호 영화시장 분석가는 '1987'의 네이버 영화 정보 네티즌 10점 평에서 언급된 단어들을 지난해 12월 27일부터 올해 1월 10일까지 통계 프로그램 R과 KoNLP 패키지로 텍스트마이닝하여 분석했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문장분리 알고리즘"
      ],
      "metadata": {
        "id": "0246XTleVnJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxDpI3oGVUEd",
        "outputId": "6f6ab9a6-0af7-4f62-d541-d9bc813c7392"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from kss) (1.7.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.13.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "s = \"회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요 다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다 강남역 맛집 토끼정의 외부 모습.\"\n",
        "\n",
        "for sent in kss.split_sentences(s):\n",
        "    print(sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI6bQ-vyVrk4",
        "outputId": "c742b50e-3f02-4922-cc88-680f481aabf1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요\n",
            "다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다 강남역 맛집 토끼정의 외부 모습.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell Check"
      ],
      "metadata": {
        "id": "xV4Kgs4LWEqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### py-hanspell 설치"
      ],
      "metadata": {
        "id": "ibDjZWBjW-MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgJOZEcRV9Us",
        "outputId": "e2bb1365-b660-4ec9-8857-a22e767af996"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-ui1rt7nk\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-ui1rt7nk\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "\n",
        "result = spell_checker.check(u'안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.')\n",
        "result.as_dict()  # dict로 출력\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPtdHdL9WI8C",
        "outputId": "77befed4-f7d0-47f6-ed97-a7cd244433be"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'checked': '안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.',\n",
              " 'errors': 4,\n",
              " 'original': '안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.',\n",
              " 'result': True,\n",
              " 'time': 0.8130576610565186,\n",
              " 'words': OrderedDict([('안녕하세요.', 2),\n",
              "              ('저는', 0),\n",
              "              ('한국인입니다.', 2),\n",
              "              ('이', 2),\n",
              "              ('문장은', 2),\n",
              "              ('한글로', 0),\n",
              "              ('작성됐습니다.', 1)])}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jggWjz8QXNtx",
        "outputId": "f8782705-9faa-46f2-b470-13a880791868"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Checked(result=True, original='안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.', checked='안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.', errors=4, words=OrderedDict([('안녕하세요.', 2), ('저는', 0), ('한국인입니다.', 2), ('이', 2), ('문장은', 2), ('한글로', 0), ('작성됐습니다.', 1)]), time=0.8130576610565186)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "\n",
        "spell_checker.check([u'안녕 하세요.', u'저는 한국인 입니다.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBOZ4oNFXSGS",
        "outputId": "5ee39738-f2f1-4b13-92c1-a69030584565"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Checked(result=True, original='안녕 하세요.', checked='안녕하세요.', errors=1, words=OrderedDict([('안녕하세요.', 2)]), time=0.20537710189819336),\n",
              " Checked(result=True, original='저는 한국인 입니다.', checked='저는 한국인입니다.', errors=1, words=OrderedDict([('저는', 0), ('한국인입니다.', 2)]), time=0.19569897651672363)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in result.words.items():\n",
        "     print(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZZYyD30Xppq",
        "outputId": "1b472f7d-0a60-4584-ab19-056cdaf4336c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요. 2\n",
            "저는 0\n",
            "한국인입니다. 2\n",
            "이 2\n",
            "문장은 2\n",
            "한글로 0\n",
            "작성됐습니다. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell.constants import CheckResult"
      ],
      "metadata": {
        "id": "DJiXedIWYaEf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 반복되는 이모티콘, 자모 normalize하는 라이브러리 -> soynlp.normalizer\n",
        "- 'ㅋㅋㅋㅋㅋㅋ', '하하하하하하' 등"
      ],
      "metadata": {
        "id": "YW5ozWFYYiGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soynlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzF6WdspY3La",
        "outputId": "3aaed9da-5bfe-4e37-cf46-89345de6018e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: soynlp in /usr/local/lib/python3.7/dist-packages (0.0.493)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.normalizer import *\n",
        "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_1nGh7_YxB1",
        "outputId": "b5e04066-013b-43d3-a7d1-a9aec3ded57f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "와하하핫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 외래어 사전 사용"
      ],
      "metadata": {
        "id": "spAE_07fZDFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmtJe5B1Y-ex",
        "outputId": "013fc8ef-5a78-42cf-a94a-aeeabdb847a7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "awk: cannot open ./cookie (No such file or directory)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 19779  100 19779    0     0  49946      0 --:--:-- --:--:-- --:--:-- 49946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lownword_map = {}\n",
        "lownword_data = open('/content/confused_loanwords.txt', 'r', encoding='utf-8')\n",
        "\n",
        "lines = lownword_data.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    miss_spell = line.split('\\t')[0]\n",
        "    ori_word = line.split('\\t')[1]\n",
        "    lownword_map[miss_spell] = ori_word"
      ],
      "metadata": {
        "id": "BFSftP1SZHI_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_check_text(texts):\n",
        "    corpus = []\n",
        "    for sent in texts:\n",
        "        spaced_text = spacing(sent)\n",
        "        spelled_sent = spell_checker.check(sent)\n",
        "        checked_sent = spelled_sent.checked\n",
        "        normalized_sent = repeat_normalize(checked_sent)\n",
        "        for lownword in lownword_map:\n",
        "            normalized_sent = normalized_sent.replace(lownword, lownword_map[lownword])\n",
        "        corpus.append(normalized_sent)\n",
        "    return corpus\n",
        "    \n",
        "spell_preprocessed_corpus = spell_check_text('나는밥을먹는다하하하하하하하')"
      ],
      "metadata": {
        "id": "QWOoz4cmZN7l"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pos Tag : 형태소 분석"
      ],
      "metadata": {
        "id": "6wNi0SURZ9Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k0ZqWVttZuoh",
        "outputId": "fe8c52c7-8009-4657-9c53-a95d39c87d20"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'khaiii' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.5)\n",
            "mkdir: cannot create directory ‘build’: File exists\n",
            "-- [khaiii] fused multiply add option enabled\n",
            "-- [hunter] Calculating Toolchain-SHA1\n",
            "-- [hunter] Calculating Config-SHA1\n",
            "-- [hunter] HUNTER_ROOT: /root/.hunter\n",
            "-- [hunter] [ Hunter-ID: 70287b1 | Toolchain-ID: 02ccb06 | Config-ID: dffbc08 ]\n",
            "-- [hunter] BOOST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.68.0-p1)\n",
            "-- Boost version: 1.68.0\n",
            "-- [hunter] CXXOPTS_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 2.1.1-pre)\n",
            "-- [hunter] EIGEN_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.5)\n",
            "-- [hunter] FMT_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 4.1.0)\n",
            "-- [hunter] GTEST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.8.0-hunter-p11)\n",
            "-- [hunter] NLOHMANN_JSON_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.0)\n",
            "-- [hunter] SPDLOG_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 0.16.3-p1)\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/build\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target obj_khaiii\u001b[0m\n",
            "[ 65%] Built target obj_khaiii\n",
            "[ 69%] Built target khaiii\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target bin_khaiii\u001b[0m\n",
            "[ 76%] Built target bin_khaiii\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target test_khaiii\u001b[0m\n",
            "[100%] Built target test_khaiii\n",
            "Built target resource\n",
            "[ 65%] Built target obj_khaiii\n",
            "[ 69%] Built target khaiii\n",
            "[ 76%] Built target bin_khaiii\n",
            "[100%] Built target test_khaiii\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Up-to-date: /usr/local/include/khaiii\n",
            "-- Up-to-date: /usr/local/include/khaiii/khaiii_dev.h\n",
            "-- Up-to-date: /usr/local/include/khaiii/KhaiiiApi.hpp\n",
            "-- Up-to-date: /usr/local/include/khaiii/khaiii_api.h\n",
            "-- Up-to-date: /usr/local/share/khaiii\n",
            "-- Up-to-date: /usr/local/share/khaiii/preanal.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/cnv2hdn.lin\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.3.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.one\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.4.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/config.json\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.len\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.5.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.key\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.tri\n",
            "-- Up-to-date: /usr/local/share/khaiii/hdn2tag.lin\n",
            "-- Up-to-date: /usr/local/share/khaiii/embed.bin\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.2.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/preanal.tri\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so.0.4\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so.0\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so\n",
            "-- Up-to-date: /usr/local/bin/khaiii\n",
            "\u001b[36mRun CPack packaging tool for source...\u001b[0m\n",
            "CPack: Create package using ZIP\n",
            "CPack: Install projects\n",
            "CPack: - Install directory: /content/khaiii\n",
            "CPack: Create package\n",
            "CPack: - package: /content/build/khaiii-0.4.zip generated.\n",
            "Built target package_python\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./build/package_python\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: khaiii\n",
            "  Building wheel for khaiii (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for khaiii: filename=khaiii-0.4-py3-none-any.whl size=22882905 sha256=a8baee9bbfcbd64d92ee42f22a8df5642c4ab45368ab1a9579af06e9ded49faf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eaw3060k/wheels/79/cb/8c/aed91c3bafdd491bf3fcbed5809b53e50a508c6e167bbbeff8\n",
            "Successfully built khaiii\n",
            "Installing collected packages: khaiii\n",
            "  Attempting uninstall: khaiii\n",
            "    Found existing installation: khaiii 0.4\n",
            "    Uninstalling khaiii-0.4:\n",
            "      Successfully uninstalled khaiii-0.4\n",
            "Successfully installed khaiii-0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "khaiii"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from khaiii import KhaiiiApi\n",
        "api = KhaiiiApi()\n",
        "\n",
        "test_sents = [\"나도 모르게 사버렸다.\"]\n",
        "\n",
        "for sent in test_sents:\n",
        "    for word in api.analyze(sent):\n",
        "        for morph in word.morphs:\n",
        "            print(morph.lex + '/' + morph.tag)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzpEmVBBaFzt",
        "outputId": "b5ce1dfa-f992-4b6f-92eb-e153e9bbc599"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나/NP\n",
            "도/JX\n",
            "모르/VV\n",
            "게/EC\n",
            "사/VV\n",
            "아/EC\n",
            "버리/VX\n",
            "었/EP\n",
            "다/EF\n",
            "./SF\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
        "\n",
        "def pos_text(texts):\n",
        "    corpus = []\n",
        "    for sent in texts:\n",
        "        pos_tagged = ''\n",
        "        for word in api.analyze(sent):\n",
        "            for morph in word.morphs:\n",
        "                if morph.tag in significant_tags:\n",
        "                    pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
        "        corpus.append(pos_tagged.strip())\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "AHvWABZViNTl"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "- 동사를 원형으로 복원"
      ],
      "metadata": {
        "id": "a60gnSZxiS_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
        "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
        "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
        "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
      ],
      "metadata": {
        "id": "l7QDGzHXiWhu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming_text(text):\n",
        "    corpus = []\n",
        "    for sent in text:\n",
        "        ori_sent = sent\n",
        "        mached_terms = re.findall(p1, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                modi_terms += lemma\n",
        "            modi_terms += '다/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "        \n",
        "        mached_terms = re.findall(p2, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                if tag != 'VX':\n",
        "                    modi_terms += lemma\n",
        "            modi_terms += '다/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "\n",
        "        mached_terms = re.findall(p3, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                modi_terms += lemma\n",
        "            if '다' != modi_terms[-1]:\n",
        "                modi_terms += '다'\n",
        "            modi_terms += '/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "\n",
        "        mached_terms = re.findall(p4, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                modi_terms += lemma\n",
        "            if '다' != modi_terms[-1]:\n",
        "                modi_terms += '다'\n",
        "            modi_terms += '/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "        corpus.append(ori_sent)\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "p5val98iidTC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords\n",
        "- 불용어 처리"
      ],
      "metadata": {
        "id": "LwE-Z4XsipZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['데/NNB', '좀/MAG', '수/NNB', '등/NNB']\n",
        "\n",
        "def remove_stopword_text(text):\n",
        "    corpus = []\n",
        "    for sent in text:\n",
        "        modi_sent = []\n",
        "        for word in sent.split(' '):\n",
        "            if word not in stopwords:\n",
        "                modi_sent.append(word)\n",
        "        corpus.append(' '.join(modi_sent))\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "pVBb561CipnZ"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}